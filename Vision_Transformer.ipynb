{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"name":"DM_HW5_20161641.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Hi0HXI_9M8lY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622978596029,"user_tz":-540,"elapsed":3509,"user":{"displayName":"SungYub Jung","photoUrl":"","userId":"06395096588766753020"}},"outputId":"0b2bf0b9-38be-4f8a-c876-8806dd4c494d"},"source":["!pip install einops"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting einops\n","  Downloading https://files.pythonhosted.org/packages/5d/a0/9935e030634bf60ecd572c775f64ace82ceddf2f504a5fd3902438f07090/einops-0.3.0-py2.py3-none-any.whl\n","Installing collected packages: einops\n","Successfully installed einops-0.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hc6X4fRDKP-j","executionInfo":{"status":"ok","timestamp":1622978597173,"user_tz":-540,"elapsed":1147,"user":{"displayName":"SungYub Jung","photoUrl":"","userId":"06395096588766753020"}}},"source":["#구현하는 모델에서 쓰이는 모든 activation함수는 정의하여 드린 GELU 함수를 사용해야함.\n","#MultiHeadAttention에서 Head로 나눌때, 이미지를 patch로자른후 sequence로 만들때 Rearrange함수를 사용하면 편리함.(사용하지 않으셔도 됩니다)\n","#CIFAR10에 대한 test accuracy가 60프로 이상인 ViT모델을 만드시오.\n","import tensorflow as tf\n","from einops.layers.tensorflow import Rearrange\n","from tensorflow.keras.activations import gelu\n","GELU = lambda x : gelu(x)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"QrJci74oNV2m","executionInfo":{"status":"ok","timestamp":1622978597493,"user_tz":-540,"elapsed":324,"user":{"displayName":"SungYub Jung","photoUrl":"","userId":"06395096588766753020"}}},"source":["#논문[1]에서 설명하는 MultiHeadAttention을 만들어라.\n","class MultiHeadedAttention(tf.keras.Model):\n","    #dimension - 모델의 dimension(MHA를 거친 후의 dimension)\n","    def __init__(self, dimension, heads=8):\n","        super(MultiHeadedAttention, self).__init__()\n","        ############Write your code Here############\n","        self.dimension = dimension\n","        self.heads = heads\n","        \n","        assert dimension % heads == 0\n","\n","        self.depth = dimension // heads\n","        \n","        self.W0_layer = tf.keras.layers.Dense(dimension)\n","        self.query_layer = tf.keras.layers.Dense(dimension)\n","        self.key_layer = tf.keras.layers.Dense(dimension)\n","        self.value_layer = tf.keras.layers.Dense(dimension)\n","        self.combine_layer = tf.keras.layers.Dense(dimension)\n","        ############################################\n","    def call(self, inputs):\n","        output = None\n","        batch_size = tf.shape(inputs)[0]\n","        ############Write your code Here############\n","        query = self.query_layer(inputs)\n","        key = self.key_layer(inputs)\n","        value = self.value_layer(inputs)\n","        \n","        query = tf.reshape(query,shape = (batch_size,-1,self.heads,self.depth))\n","        key = tf.reshape(key,shape = (batch_size,-1,self.heads,self.depth))\n","        value = tf.reshape(value,shape = (batch_size,-1,self.heads,self.depth))\n","        tf.transpose(query, perm=[0, 2, 1, 3])\n","        tf.transpose(key, perm=[0, 2, 1, 3])\n","        tf.transpose(value, perm=[0, 2, 1, 3])\n","\n","        score = tf.matmul(query, key, transpose_b=True)\n","        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n","        scaled_score = score / tf.math.sqrt(dim_key)\n","        weights = GELU(scaled_score)\n","        attention = tf.matmul(weights, value)\n","        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n","\n","        concat_attention = tf.reshape(attention, (batch_size, -1, self.dimension))\n","        output = self.combine_layer(concat_attention)\n","        ############################################\n","        return output\n","\n","#인자로 받은 residual_function을 사용하여 real_function값을 return하여주는 Class를 만들어라.(call함수 참고)\n","class ResidualBlock(tf.keras.Model):\n","    def __init__(self, residual_function):\n","        super(ResidualBlock, self).__init__()\n","        ############Write your code Here############\n","        self.residual_function = residual_function\n","        ############################################\n","\n","    def call(self, inputs):\n","        return self.residual_function(inputs) + inputs\n","\n","#인자로 받은 normfunction에 들어가기전에 LayerNormalization을 해주는 Class를 만들어라.(call함수 참고)\n","class NormalizationBlock(tf.keras.Model):\n","    def __init__(self, norm_function, epsilon=1e-5):\n","        super(NormalizationBlock, self).__init__()\n","        ############Write your code Here############\n","        self.norm_function = norm_function\n","        self.normalize = tf.keras.layers.LayerNormalization(epsilon =  epsilon)\n","        ############################################\n","\n","    def call(self, inputs):\n","        return self.norm_function(self.normalize(inputs))\n","\n","#논문[1]에서의 MLPBlock을 만들어라.\n","class MLPBlock(tf.keras.Model):\n","    #output_dimension - MLPBlock의 output dimension\n","    #hidden_dimension - MLPBlock의 hidden layer dimension\n","    def __init__(self, output_dimension, hidden_dimension):\n","        super(MLPBlock, self).__init__()\n","        ############Write your code Here############\n","        self.layer1 = tf.keras.layers.Dense(hidden_dimension)\n","        self.GELU = GELU\n","        self.layer2 = tf.keras.layers.Dense(output_dimension)\n","        ############################################\n","\n","    def call(self, inputs):\n","        output = None\n","        ############Write your code Here############\n","        output = self.layer1(inputs)\n","        output = self.GELU(output)\n","        output = self.layer2(output)\n","        ############################################\n","        return output\n","\n","#논문[1]을 읽고 TransformerEncoder를 위에서 정의한 class들을 사용하여 만들어라.\n","class TransformerEncoder(tf.keras.Model):\n","    #dimension - 모델의 dimension(MHA를 거친 후의 dimension), heads - MHA에서 head의 개수\n","    #depth - encoder layer의 개수, mlp_dimension - MLP block의 hidden layer의 dimension\n","    def __init__(self, dimension, depth, heads, mlp_dimension): \n","        super(TransformerEncoder, self).__init__()\n","        layers_ = []\n","        for _ in range(depth):\n","            ############Write your code Here############\n","            layers_.append(ResidualBlock(NormalizationBlock(MultiHeadedAttention(dimension, heads))))\n","            layers_.append(ResidualBlock(NormalizationBlock(MLPBlock(dimension,mlp_dimension))))\n","            ############################################\n","        self.layers_ = tf.keras.Sequential(layers_)\n","\n","    def call(self, inputs):\n","        return self.layers_(inputs)\n","\n","#논문[2]를 읽고 ViT모델을 위에서 정의한 class들을 사용하여 만들어라.\n","class ImageTransformer(tf.keras.Model):\n","    #image_size - 이미지의 W==H의 크기(int), patch_size - 이미지를 쪼갤 patch의 크기(int)\n","    #n_classes - 최종 class의 개수, batch_size - 배치사이즈\n","    #dimension - 모델의 dimension(MHA를 거친 후의 dimension), depth - encoder layer의 개수\n","    #heads - MHA에서 head의 개수, mlp_dimension - MLP block의 hidden layer의 dimension\n","    #channel - input image에 대한 channel의 수\n","    def __init__(\n","            self, image_size, patch_size, n_classes, batch_size,\n","            dimension, depth, heads, mlp_dimension, channels=3):\n","        super(ImageTransformer, self).__init__()\n","        assert image_size % patch_size == 0, 'invalid patch size for image size'\n","\n","        num_patches = (image_size // patch_size) ** 2\n","        self.patch_size = patch_size\n","        self.dimension = dimension\n","        self.batch_size = batch_size\n","\n","        self.positional_embedding = self.add_weight(\n","            \"position_embeddings\", shape=[1, num_patches + 1, dimension],\n","            initializer=tf.keras.initializers.RandomNormal(), dtype=tf.float32\n","        )\n","        self.classification_token = self.add_weight(\n","            \"classification_token\", shape=[1, 1, dimension],\n","            initializer=tf.keras.initializers.RandomNormal(), dtype=tf.float32\n","        )\n","        ############Write your code Here############\n","        self.image_size = image_size\n","        self.channels = channels\n","        self.patch_dim = channels * patch_size ** 2\n","        self.patch_projection = tf.keras.layers.Dense(dimension)\n","        self.encoder = TransformerEncoder(dimension, depth, heads, mlp_dimension)\n","        self.mlp_head = tf.keras.layers.Dense(n_classes)\n","        ############################################\n","\n","    def call(self, inputs):\n","        output = None\n","        ############Write your code Here############\n","        batch_size = tf.shape(inputs)[0]\n","        inputs = tf.reshape(inputs,[batch_size,self.image_size,self.image_size,self.channels])\n","        patches = tf.image.extract_patches(\n","            images=inputs,\n","            sizes=[1, self.patch_size, self.patch_size, 1],\n","            strides=[1, self.patch_size, self.patch_size, 1],\n","            rates=[1, 1, 1, 1],\n","            padding=\"VALID\",\n","        )\n","        patches = tf.reshape(patches, [batch_size, -1, self.patch_dim])\n","        X = self.patch_projection(patches)\n","        class_token = tf.broadcast_to(self.classification_token,[batch_size,1,self.dimension])\n","        X = tf.concat([class_token,X],axis = 1)\n","        X = X + self.positional_embedding\n","        X = self.encoder(X)\n","        output = self.mlp_head(X[:,0])\n","        ############################################\n","        return output"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"cAydwOELeFba","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622979319361,"user_tz":-540,"elapsed":630481,"user":{"displayName":"SungYub Jung","photoUrl":"","userId":"06395096588766753020"}},"outputId":"acabdc01-0cda-4d11-8ca9-ed4a6dd0b936"},"source":["from tensorflow.keras import datasets\n","# Download and prepare the CIFAR10 dataset\n","(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n","# Normalize pixel values to be between 0 and 1\n","############Write your code Here############\n","train_images = train_images / 255.0\n","test_images = test_images / 255.0\n","############################################\n","# Make image shape (BS, H, W, C) to (BS, C, H, W)\n","############Write your code Here############\n","train_images = train_images.reshape(-1,3,32,32)\n","test_images = test_images.reshape(-1,3,32,32)\n","############################################\n","\n","#Initialize your model\n","#Initialize optimizer and loss and compile it to the model\n","############Write your code Here############\n","\n","model = ImageTransformer(image_size=32, patch_size=4, n_classes=10, batch_size=32, dimension=64, depth=6, heads=4, mlp_dimension=128, channels=3)\n","model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              optimizer=\"adam\",\n","              metrics=[\"accuracy\"])\n","early_stop = tf.keras.callbacks.EarlyStopping(patience=5)\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n","\n","############################################\n","\n","#Train your model\n","############Write your code Here############\n","model.fit(train_images, train_labels, batch_size=32, epochs=100, validation_data = (test_images, test_labels), callbacks=[early_stop,reduce_lr])\n","############################################\n","print('==============Training Finished===============')\n","\n","#Evaluate your test samples\n","accuracy = 0\n","############Write your code Here############\n","_, accuracy = model.evaluate(test_images, test_labels, batch_size=32)\n","############################################\n","\n","print('Test Accuracy :', accuracy)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","1563/1563 [==============================] - 45s 24ms/step - loss: 1.9107 - accuracy: 0.3134 - val_loss: 1.6408 - val_accuracy: 0.3971\n","Epoch 2/100\n","1563/1563 [==============================] - 37s 23ms/step - loss: 1.5328 - accuracy: 0.4430 - val_loss: 1.5282 - val_accuracy: 0.4420\n","Epoch 3/100\n","1563/1563 [==============================] - 36s 23ms/step - loss: 1.3614 - accuracy: 0.5097 - val_loss: 1.3616 - val_accuracy: 0.5051\n","Epoch 4/100\n","1563/1563 [==============================] - 35s 23ms/step - loss: 1.2429 - accuracy: 0.5521 - val_loss: 1.2628 - val_accuracy: 0.5391\n","Epoch 5/100\n","1563/1563 [==============================] - 37s 24ms/step - loss: 1.1635 - accuracy: 0.5832 - val_loss: 1.2013 - val_accuracy: 0.5666\n","Epoch 6/100\n","1563/1563 [==============================] - 36s 23ms/step - loss: 1.0965 - accuracy: 0.6059 - val_loss: 1.1853 - val_accuracy: 0.5779\n","Epoch 7/100\n","1563/1563 [==============================] - 36s 23ms/step - loss: 1.0367 - accuracy: 0.6286 - val_loss: 1.1726 - val_accuracy: 0.5857\n","Epoch 8/100\n","1563/1563 [==============================] - 36s 23ms/step - loss: 0.9814 - accuracy: 0.6469 - val_loss: 1.1191 - val_accuracy: 0.6014\n","Epoch 9/100\n","1563/1563 [==============================] - 36s 23ms/step - loss: 0.9287 - accuracy: 0.6680 - val_loss: 1.1353 - val_accuracy: 0.6079\n","Epoch 10/100\n","1563/1563 [==============================] - 36s 23ms/step - loss: 0.8822 - accuracy: 0.6836 - val_loss: 1.1431 - val_accuracy: 0.6063\n","Epoch 11/100\n","1563/1563 [==============================] - 36s 23ms/step - loss: 0.8304 - accuracy: 0.7030 - val_loss: 1.1230 - val_accuracy: 0.6182\n","Epoch 12/100\n","1563/1563 [==============================] - 36s 23ms/step - loss: 0.5763 - accuracy: 0.7982 - val_loss: 1.1108 - val_accuracy: 0.6357\n","Epoch 13/100\n","1563/1563 [==============================] - 37s 23ms/step - loss: 0.4947 - accuracy: 0.8296 - val_loss: 1.1465 - val_accuracy: 0.6349\n","Epoch 14/100\n","1563/1563 [==============================] - 36s 23ms/step - loss: 0.4469 - accuracy: 0.8464 - val_loss: 1.1983 - val_accuracy: 0.6312\n","Epoch 15/100\n","1563/1563 [==============================] - 37s 23ms/step - loss: 0.4065 - accuracy: 0.8621 - val_loss: 1.2389 - val_accuracy: 0.6278\n","Epoch 16/100\n","1563/1563 [==============================] - 37s 23ms/step - loss: 0.3463 - accuracy: 0.8897 - val_loss: 1.2632 - val_accuracy: 0.6289\n","Epoch 17/100\n","1563/1563 [==============================] - 36s 23ms/step - loss: 0.3388 - accuracy: 0.8922 - val_loss: 1.2790 - val_accuracy: 0.6292\n","==============Training Finished===============\n","313/313 [==============================] - 4s 13ms/step - loss: 1.2790 - accuracy: 0.6292\n","Test Accuracy : 0.6291999816894531\n"],"name":"stdout"}]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"name":"DM_Assignment_5.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Hi0HXI_9M8lY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622096231189,"user_tz":-540,"elapsed":2651,"user":{"displayName":"BokJin Chung","photoUrl":"","userId":"00013653044591416632"}},"outputId":"7862d8a5-07f4-4e67-a77b-4809b18144ba"},"source":["!pip install einops"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: einops in /usr/local/lib/python3.7/dist-packages (0.3.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hc6X4fRDKP-j"},"source":["#구현하는 모델에서 쓰이는 모든 activation함수는 정의하여 드린 GELU 함수를 사용해야함.\n","#MultiHeadAttention에서 Head로 나눌때, 이미지를 patch로자른후 sequence로 만들때 Rearrange함수를 사용하면 편리함.(사용하지 않으셔도 됩니다)\n","#CIFAR10에 대한 test accuracy가 60프로 이상인 ViT모델을 만드시오.\n","import tensorflow as tf\n","from einops.layers.tensorflow import Rearrange\n","from tensorflow.keras.activations import gelu\n","GELU = lambda x : gelu(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QrJci74oNV2m"},"source":["#논문[1]에서 설명하는 MultiHeadAttention을 만들어라.\n","class MultiHeadedAttention(tf.keras.Model):\n","    #dimension - 모델의 dimension(MHA를 거친 후의 dimension)\n","    def __init__(self, dimension, heads=8):\n","        super(MultiHeadedAttention, self).__init__()\n","        ############Write your code Here############\n","        \n","        ############################################\n","    def call(self, inputs):\n","        output = None\n","        ############Write your code Here############\n","        \n","        ############################################\n","        return output\n","\n","#인자로 받은 residual_function을 사용하여 real_function값을 return하여주는 Class를 만들어라.(call함수 참고)\n","class ResidualBlock(tf.keras.Model):\n","    def __init__(self, residual_function):\n","        super(ResidualBlock, self).__init__()\n","        ############Write your code Here############\n","        \n","        ############################################\n","\n","    def call(self, inputs):\n","        return self.residual_function(inputs) + inputs\n","\n","#인자로 받은 normfunction에 들어가기전에 LayerNormalization을 해주는 Class를 만들어라.(call함수 참고)\n","class NormalizationBlock(tf.keras.Model):\n","    def __init__(self, norm_function, epsilon=1e-5):\n","        super(NormalizationBlock, self).__init__()\n","        ############Write your code Here############\n","        \n","        ############################################\n","\n","    def call(self, inputs):\n","        return self.norm_function(self.normalize(inputs))\n","\n","#논문[1]에서의 MLPBlock을 만들어라.\n","class MLPBlock(tf.keras.Model):\n","    #output_dimension - MLPBlock의 output dimension\n","    #hidden_dimension - MLPBlock의 hidden layer dimension\n","    def __init__(self, output_dimension, hidden_dimension):\n","        super(MLPBlock, self).__init__()\n","        ############Write your code Here############\n","        \n","        ############################################\n","\n","    def call(self, inputs):\n","        output = None\n","        ############Write your code Here############\n","        \n","        ############################################\n","        return output\n","\n","#논문[1]을 읽고 TransformerEncoder를 위에서 정의한 class들을 사용하여 만들어라.\n","class TransformerEncoder(tf.keras.Model):\n","    #dimension - 모델의 dimension(MHA를 거친 후의 dimension), heads - MHA에서 head의 개수\n","    #depth - encoder layer의 개수, mlp_dimension - MLP block의 hidden layer의 dimension\n","    def __init__(self, dimension, depth, heads, mlp_dimension): \n","        super(TransformerEncoder, self).__init__()\n","        layers_ = []\n","        for _ in range(depth):\n","            ############Write your code Here############\n","            \n","            ############################################\n","        self.layers_ = tf.keras.Sequential(layers_)\n","\n","    def call(self, inputs):\n","        return self.layers_(inputs)\n","\n","#논문[2]를 읽고 ViT모델을 위에서 정의한 class들을 사용하여 만들어라.\n","class ImageTransformer(tf.keras.Model):\n","    #image_size - 이미지의 W==H의 크기(int), patch_size - 이미지를 쪼갤 patch의 크기(int)\n","    #n_classes - 최종 class의 개수, batch_size - 배치사이즈\n","    #dimension - 모델의 dimension(MHA를 거친 후의 dimension), depth - encoder layer의 개수\n","    #heads - MHA에서 head의 개수, mlp_dimension - MLP block의 hidden layer의 dimension\n","    #channel - input image에 대한 channel의 수\n","    def __init__(\n","            self, image_size, patch_size, n_classes, batch_size,\n","            dimension, depth, heads, mlp_dimension, channels=3):\n","        super(ImageTransformer, self).__init__()\n","        assert image_size % patch_size == 0, 'invalid patch size for image size'\n","\n","        num_patches = (image_size // patch_size) ** 2\n","        self.patch_size = patch_size\n","        self.dimension = dimension\n","        self.batch_size = batch_size\n","\n","        self.positional_embedding = self.add_weight(\n","            \"position_embeddings\", shape=[num_patches + 1, dimension],\n","            initializer=tf.keras.initializers.RandomNormal(), dtype=tf.float32\n","        )\n","        self.classification_token = self.add_weight(\n","            \"classification_token\", shape=[1, 1, dimension],\n","            initializer=tf.keras.initializers.RandomNormal(), dtype=tf.float32\n","        )\n","        ############Write your code Here############\n","        \n","        ############################################\n","\n","    def call(self, inputs):\n","        output = None\n","        ############Write your code Here############\n","        \n","        ############################################\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cAydwOELeFba"},"source":["from tensorflow.keras import datasets\n","# Download and prepare the CIFAR10 dataset\n","(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n","# Normalize pixel values to be between 0 and 1\n","############Write your code Here############\n","\n","############################################\n","# Make image shape (BS, H, W, C) to (BS, C, H, W)\n","############Write your code Here############\n","\n","############################################\n","\n","#Initialize your model\n","#Initialize optimizer and loss and compile it to the model\n","############Write your code Here############\n","\n","############################################\n","\n","#Train your model\n","############Write your code Here############\n","\n","############################################\n","print('==============Training Finished===============')\n","\n","#Evaluate your test samples\n","accuracy = 0\n","############Write your code Here############\n","\n","############################################\n","\n","print('Test Accuracy :', accuracy)"],"execution_count":null,"outputs":[]}]}